{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的Python库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from Data import Data\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置__可视化参数__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (20, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化__随机种子__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置默认__浮点数类型__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alibaba dataset\n",
    "# data = dict()\n",
    "# for filename in os.listdir('.'):\n",
    "#     if filename.endswith('.pickle'):\n",
    "#         with open(filename, 'rb') as f:\n",
    "#             data[filename[filename.rindex('_') + 1:filename.index('.')]] = pickle.load(f)[3000:]\n",
    "# data = pd.DataFrame(data)\n",
    "# data.head(5)\n",
    "\n",
    "# Bustracker Dataset\n",
    "data = pd.read_csv('BusTrackerData/1.csv',names=['date', 'freq'])\n",
    "# data = pd.read_csv('SDSS/DTW/149.csv',names=['date', 'freq'])\n",
    "data = data.dropna(axis=0,how='any')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "# df.reset_index(drop=True,inplace=True)\n",
    "data = data.set_index('date')\n",
    "\n",
    "# SDSS Dataset\n",
    "# data = pd.read_csv('SDSS/DTW149.csv',names=['date', 'freq'])\n",
    "# data = data.dropna(axis=0,how='any')\n",
    "# data['date'] = pd.to_datetime(data['date'])\n",
    "# # df.reset_index(drop=True,inplace=True)\n",
    "# data = data.set_index('date')\n",
    "\n",
    "    \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择__预测目标__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column in enumerate(data.columns):\n",
    "    print('{}: {}'.format(i, column))\n",
    "# target = int(input('target (0~{}):'.format(len(data.columns) - 1)))\n",
    "target = 0\n",
    "assert 0 <= target < len(data.columns)\n",
    "print('Your choice:', data.columns[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆分__训练数据__和__验证数据__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_SPLIT = 800\n",
    "TRAIN_SPLIT = 7000  # Bustracker\n",
    "# TRAIN_SPLIT = 4000  # SDSS\n",
    "\n",
    "UNI_DATA = True\n",
    "HISTORY_SIZE, TARGET_SIZE, STEP, SINGLE_STEP = 30, 5, 1, True\n",
    "data = Data(data.values, target, TRAIN_SPLIT, HISTORY_SIZE, TARGET_SIZE, UNI_DATA, STEP, SINGLE_STEP)\n",
    "print(data.x_train.shape, data.x_train_target.shape, data.y_train.shape)\n",
    "# print(data.x_train[0])\n",
    "# print(data.y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义__Generator模型__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units=16, return_sequences=False, input_shape=data.x_train.shape[-2:]),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1 if SINGLE_STEP else TARGET_SIZE)\n",
    "])\n",
    "\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义__Discriminator模型__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_discriminator(data):\n",
    "    x_input = tf.keras.Input(shape=data.x_train_target.shape[-2:], name='history')\n",
    "    y_input = tf.keras.Input(shape=(1 if SINGLE_STEP else TARGET_SIZE,), name='target')\n",
    "    y_reshape = tf.keras.layers.Reshape((1, 1 if SINGLE_STEP else TARGET_SIZE))(y_input)\n",
    "\n",
    "    series = tf.keras.layers.Concatenate(axis=1)([x_input, y_reshape])\n",
    "    series = tf.keras.layers.LSTM(32, return_sequences=False)(series)\n",
    "#     series = tf.keras.layers.Flatten()(series)\n",
    "    decision = tf.keras.layers.Dense(1)(series)\n",
    "#     decision = tf.keras.activations.sigmoid(decision)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[x_input, y_input], outputs=[decision])\n",
    "\n",
    "discriminator = make_discriminator(data)\n",
    "\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义__损失函数__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output, y, y_pred):\n",
    "#     mse = tf.keras.losses.MeanSquaredError()\n",
    "#     return cross_entropy(tf.ones_like(fake_output), fake_output) + mse(y, y_pred)\n",
    "\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, y, y_pred):\n",
    "#     mse = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "#     return real_loss + fake_loss + mse(y, y_pred)\n",
    "    \n",
    "    return real_loss + fake_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置__优化器__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义__训练步__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, x_target, y, genTrain):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        y_pred = generator(x, training=True)\n",
    "\n",
    "        real_output = discriminator([x_target, y], training=True)\n",
    "        fake_output = discriminator([x_target, y_pred], training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output, y, y_pred)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, y, y_pred)\n",
    "        \n",
    "        if genTrain:\n",
    "            gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义__评估函数__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def evaluate(model, data):\n",
    "    y_true, y_pred = [], []\n",
    "    for (x, x_target, y) in data:\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend(model(x))\n",
    "    return mse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义__训练循环__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(data_train, data_val, epochs, steps_per_epoch):\n",
    "#     loss_history = {'train_loss': [], 'val_loss': []}\n",
    "#     final_val_loss = 1000\n",
    "#     pp = 0\n",
    "#     for epoch in range(epochs):\n",
    "#         start = time.time()\n",
    "\n",
    "#         gen_loss, disc_loss, train_loss, val_loss = None, None, None, None\n",
    "#         for step, (x, x_target, y) in enumerate(data_train.repeat()):\n",
    "#             if step >= steps_per_epoch:\n",
    "#                 break\n",
    "#             if step % 6 == 0:\n",
    "#                 genTrain = True\n",
    "#             else:\n",
    "#                 genTrain = True\n",
    "#             gen_loss, disc_loss = train_step(x, x_target, y, genTrain)\n",
    "#         train_loss = evaluate(generator, data_train)\n",
    "#         val_loss = evaluate(generator, data_val)\n",
    "        \n",
    "#         if val_loss < final_val_loss:\n",
    "#             final_val_loss = val_loss\n",
    "#             generator.save('Model/BusTracker/'+ 'GAN-'+str(TARGET_SIZE)+'.h5')\n",
    "#             print('new model save', pp)\n",
    "#             pp = pp + 1\n",
    "            \n",
    "#         loss_history['train_loss'].append(train_loss)\n",
    "#         loss_history['val_loss'].append(val_loss)\n",
    "\n",
    "#         print('Time for epoch {} is {:.3f} sec. gen_loss: {:.6f}, disc_loss: {:.6f}, train_loss: {:.6f}, val_loss: {:.6f}'.format(\n",
    "#             epoch + 1, time.time() - start, gen_loss, disc_loss, train_loss, val_loss\n",
    "#         ))\n",
    "\n",
    "#     return loss_history\n",
    "\n",
    "def train(data_train, data_val, epochs, steps_per_epoch):\n",
    "    loss_history = {'train_loss': [], 'val_loss': []}\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        gen_loss, disc_loss, train_loss, val_loss = None, None, None, None\n",
    "        for step, (x, x_target, y) in enumerate(data_train.repeat()):\n",
    "            if step >= steps_per_epoch:\n",
    "                break\n",
    "            if step % 6 == 0:\n",
    "                genTrain = True\n",
    "            else:\n",
    "                genTrain = True\n",
    "            gen_loss, disc_loss = train_step(x, x_target, y, genTrain)\n",
    "        train_loss = evaluate(generator, data_train)\n",
    "        val_loss = evaluate(generator, data_val)\n",
    "\n",
    "        loss_history['train_loss'].append(train_loss)\n",
    "        loss_history['val_loss'].append(val_loss)\n",
    "\n",
    "        print('Time for epoch {} is {:.3f} sec. gen_loss: {:.6f}, disc_loss: {:.6f}, train_loss: {:.6f}, val_loss: {:.6f}'.format(\n",
    "            epoch + 1, time.time() - start, gen_loss, disc_loss, train_loss, val_loss\n",
    "        ))\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装__训练数据集__和__验证数据集__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "data_train, data_val = data.dataset(BUFFER_SIZE, BATCH_SIZE)\n",
    "# print('hhh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型__训练__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "STEPS_PER_EPOCH = 50 \n",
    "train_history = train(data_train, data_val, EPOCHS, STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型__评估__："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = evaluate(generator, data_val)\n",
    "print('Evaluation MSE LOSS:', final_loss.numpy())\n",
    "generator.save('Model/BusTracker/'+ 'GAN_Attention-'+str(TARGET_SIZE)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__损失函数__变化趋势："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "    train_loss = history['train_loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs = range(len(train_loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_history(train_history, 'Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果展示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = generator(data.x_val)\n",
    "plt.plot(data.y_val, 'b-', label='actual')\n",
    "plt.plot(y_predict, 'r--', label='predict')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
